\chapter{Colored Petri net-based Modeling Methodology}
\label{chapter:modeling}

\section{Problem Statement}

Consider a set of mixed-criticality component-based applications that are distributed and deployed across a cluster of embedded computing nodes. Each component has a set of interfaces that it exposes to other components and to the underlying framework. Once deployed, each component functions by executing operations observed on its component message queue. Each component is associated with a single executor thread that handles these operation requests. The nature of mixed-criticality means that these executor threads are scheduled in conjunction with a known set of highly critical system threads and other low priority \emph{best-effort} threads. All scheduled threads are also subject to a temporally partitioned scheduling scheme. 

\paragraph{System Assumptions:}
\begin{enumerate}
	\item Knowledge about the component definition i.e. the properties of all ports and timers in each component e.g. priority of the operations, periodicity of the timer, operations bound to timers, servers and subscribers. 
	\item Knowledge about the mapping between ports/timers and component operations i.e. how each component functionality is exposed to the environment.
	\item Knowledge about the sequence of computation \emph{steps} of finite duration that are executed inside each component operation. This is dependent on the operation's business-logic code written by the application developer.	
	\item Knowledge of worst-case time taken by the computational steps. There are some exceptions to this assumption e.g. blocking times on RMI calls cannot be accurately judged as these times are dependent on too many external factors e.g. the nature of the process scheduling on both the client and the server, the business logic of the server operation that could include RMI calls to other remote servers etc.
	\item Knowledge about the component assembly i.e. the connections between instantiated components that forms communication topologies.
	\item Knowledge about the mapping between application processes and hosts i.e. the actual embedded computer on which the process will execute.
	\item Knowledge about the temporal partitioning schedule enforced by the operating system on each host.
	\item Knowledge about the mapping between application processes and temporal partitions on each host.
\end{enumerate}

Using these assumptions, the problem here is to ensure that the temporal behavior of the composed system meets its end-to-end timing requirements e.g. trigger-to-response times between distant sensors and actuators. Providing this guarantee implicitly requires that communicating components in a component assembly meet individual timing deadlines. Following the DREMS component model execution, a blocking I/O operation blocks a component from attending to any other requests till the operation is completed. Such blocking interaction patterns can propagate large delays to other components, especially in a highly connected system. A useful analysis result here would not only be in identifying end-to-end timing violations but also tracing delays within individual components. Tracking timing violations enables the analysis in identifying the causes for the anomalies e.g. nontrivial circular dependencies or scheduling delays. If an abstract model of the business logic of component operations is also encoded in the analysis model, then inefficient coding practices such as wasteful loops can also be marked as probable causes for deadline violations. 

Individual components need to be analyzed to identify the \emph{pure execution times} of the various computational steps in the component operations i.e. the amount of time taken by the various steps in an operation to complete if the corresponding component executor thread was the only thread executing on the system with zero CPU contention. When a set of tested components are composed together, each component's execution is affected by various factors including scheduling delays, network communication delays, blocking delays and other interaction-specific variabilities. Any timing analysis model for component-based software should account for such factors. 

There are two important challenges to modeling and analyzing DRE systems: scope and abstraction level. The scope of the analysis here should be the full system of composed components. The abstraction level of the analysis must include enough detail to account for the various timing delays mentioned above while also not capturing all aspects of low-level code. A highly detailed and dynamic low-level model is necessary for simulation but not ideal for model checking and verification-based analysis due to issues like state space explosion. Also, highly composable system designs provide recombinant components that can be selected and assembled in various combinations to satisfy user requirements. In such cases, the analysis model must be efficiently capable of tackling changes in component assembly e.g. moving components to separate processes or devices, adding or removing additional instances of the same component etc. This is a challenge when building and non-trivially generating an analysis model from a system design. Thus, efficiency, scalability and extensibility are also modeling requirements for our timing analysis.

\section{Colored Petri Net-based Analysis Model}

Petri nets have been introduced in Section \ref{sec:petri_nets}. Ordinary Petri nets have no types and are not modular. Tokens are abstract dots that represent the presence or absence of some entity. Tokens in a place could represent the presence of a message, or availability of resource i.e. the state of execution of the place that is holding the token. The number of tokens in a place could therefore be used to represent the quantity of some available resource. Also, ordinary Petri nets are flat structures; no hierarchy can be established to make the model more readable or concise. 

With Colored Petri nets (CPN) \cite{CPN}, it is possible to use data types and complex data manipulations -- each token has attached a typed data value, called the token \emph{color}. It is also possible to make hierarchical descriptions i.e. a large model can be obtained by combining a set of submodels with well-defined interfaces between submodels and well-defined semantics of the combined model. Furthermore, each submodel can be reused. One of the primary reasons for choosing Colored Petri Nets over other high-level Petri Nets such as Timed Petri Nets or other modeling paradigms like Timed Automata is because of the powerful modeling concepts made available by token colors. Each \emph{colored token} can be a heterogeneous data structure such as a \emph{record} that can contain an arbitrary number of fields. This enables modeling within a single \emph{color-set} (C-style \texttt{struct}) system properties such as temporal partitioning, component interaction patterns, and even distributed deployment. The token colors can be inspected, modified, and manipulated by the occurring transitions and the arc bindings. Component properties such as thread priority, port connections and real-time requirements can be easily encoded into a single colored token, making the model considerably concise. 

The CPN analysis model, as modeled in CPN Tools \cite{CPNTools}, is shown in Figure \ref{fig:hlcpn}. Places, shown as ovals, in this model contain colored (typed) tokens that represent the state of interest for analysis e.g. \emph{Clocks} place holds tokens of type \emph{clocks} maintaining information regarding the state of the clock values and temporal partition schedule on all computing nodes. \emph{Transitions}, shown as rectangular boxes, are responsible for executing this model, progressing the state of the modeled system and transferring tokens between places. \emph{Arcs}, between transitions and places dictate the token flow and data structure manipulations. All arcs contain \emph{inscriptions}, which are essentially function calls, written in Standard ML \cite{milner1997definition}, that manipulate token structures e.g. arc inscriptions in the arc from the transition \emph{Timer\_Expiry} to the place \emph{Timers}, manipulate all timer tokens by updating the timer expiry offsets. When a transition fires, every input arc to this transition is evaluated first. Using the input tokens and the arc inscriptions on all output arcs, tokens are released to all output places i.e. all input arcs are associated with token values before the transition fires and output arc tokens are calculated after the transition fires. This process of associating a token value to an arc is called an \emph{arc binding}. So, every transition firing leads to the production of strict arc bindings used to (1) evaluate guard conditions on the transitions and (2) produce output tokens.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{hlcpn_cropped}
	\caption{Colored Petri Net Analysis Model}
	\label{fig:hlcpn}
\end{figure}

From the design model of the system, we generate the initial CPN tokens that are injected into places in this analysis model. The modeling concepts in Figure \ref{fig:hlcpn} can be divided and categorized based on system-level concepts being analyzed. Figure \ref{fig:hlcpn_structure} shows the organizational structure of this CPN. Below, we describe each of these structural divisions in detail. 

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{hlcpn_structure}
	\caption{Analysis Model - Structural Aspects}
	\label{fig:hlcpn_structure}
\end{figure}

\subsection{Model of Time}
\label{sec:model_of_time}

Appropriate choice for temporal resolution is a necessary first step in order to model and analyze threads running on a processor. The OS scheduler enforces temporal partitioning and uses a priority-based scheme for threads active within a temporal partition. If there is no active OS-level temporal partitioning, the analysis model assumes the presence of a single infinitely long temporal partition in which all the application processes can execute. If multiple threads have the same priority, a round-robin (RR) scheduling is used. Each thread has a \emph{quantum} which is effectively a duration of time where the thread is allowed to keep hold of the CPU if the thread remains runnable and the scheduler determines that no other thread needs to run on that CPU instead. Thread quanta are generally defined in terms of some number of clock ticks. If it doesn't otherwise cease to be runnable, the scheduler decides whether to preempt the currently running thread every clock tick. In order to observe and analyze this behavior, we have chosen the temporal resolution to be 1 us; a fraction of 1 scheduler clock tick. In Section \ref{handling_time}, we describe the disadvantages of managing time as a fixed-step increasing variable and describe our solutions that significantly improve the generated state space and the efficiency of the analysis. 

%\subsubsection{Dynamic Time Progression:}
%Although the time resolution at the start of the analysis is chosen to be 1 ms, this is not a constant throughout the execution of the analysis model. If it were, then \emph{S} seconds of activity will generate a state space of size: $SS_{size} = \sum\limits_{i=1}^{S*1000} TF_{t_i}$
%where $TF_{t_i}$ is the number of state-changing CPN transition firings between $t_i$ and $t_{i+1}$. This large state space includes intervals of time where there is no thread activity to analyze either due to lack of operation requests, lack of ready threads for scheduling, or due to temporal partitioning. During such idle periods, the analysis engine dynamically increases the time-step size and progresses time either to (1) the next node-specific clock tick, (2) the next global timer expiry offset, (3) the next operation enqueue time stamp, (4) thread lifecycle change or (5) the next node-specific temporal partition (whichever is earliest and most relevant). This ensures that the generated state space tree is devoid of nodes where there is no thread activity. Such dynamic control of time using global variables during analysis is also one of the advantages of using Colored Petri nets. 

\subsection{Modeling Temporal Partitioning}
The place \emph{Clocks} in Figure \ref{fig:hlcpn} holds the state of the node-specific global clocks. The temporal partition schedule modeled by these clocks enforces a constraint: component operations can be scheduled and component threads can be run only when their parent partition is active. Each clock token \emph{NC} is modeled as a 3-tuple:

\vspace{-0.15in}
\begin{equation}
\label{eq:NC}
NC = \ < Node_{NC}, \ Value_{NC}, \ TPS_{Node_{NC}} >
\end{equation}

where, $Node_{NC}$ is the name of the computing node, $Value_{NC}$ is an integer representing the value of the global clock and $TPS_{Node_{NC}}$ is the temporal partition schedule on $Node_{NC}$. Each \emph{TPS} is an ordered list of temporal partitions.

\vspace{-0.15in}
\begin{equation}
\label{eq:TP}
TP = \ < Name_{TP}, \ Prd_{TP}, \ Dur_{TP}, \ Off_{TP}, \ Exec_{TP} >
\end{equation}

Each partition $TP$ (Eq. \ref{eq:TP}) is modeled as a record color-set consisting of a name $Name_{TP}$, a period $Prd_{TP}$, a duration $Dur_{TP}$, an offset $Off_{TP}$ and the state variable  $Exec_{TP}$. Aggregate of such partitions can fully describe a partition schedule. Complete partition schedules are maintained per computing node. Figure \ref{fig:tps_dt} shows a C++-style struct representation of the temporal partition schedule. Each clock structure is modeled similarly in Stardard ML as a color and these colored tokens are placed in the \emph{Clocks} place (Figure \ref{fig:hlcpn}). 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{temporal_partition_schedule}
	\caption{Temporal Partition Schedule Data Structure}
	\label{fig:tps_dt}
\end{figure}

\subsection{Modeling Component Thread Behavior}

Figure \ref{fig:Thread_Execution} presents a snippet of our CPN, modeling the thread execution cycle. The place \emph{Components} holds tokens that keep track of all the ready threads in each computing node. Each component thread $CT$ is a record characterized by:

\begin{equation}
\label{eq:CT}
CT = \ <ID_{CT}, \ Prio_{CT}, \ O_{CT}>
\end{equation}

where $ID_{CT}$ constitutes the concatenation of strings required to identify a component thread in CPN (i.e. component name, node name and partition). Every thread is characterized by a priority ($Prio_{CT}$) which is used by the OS scheduler to schedule the thread. Component threads are maintained in a priority queue in the CPN model. When scheduling a thread, a \emph{Schedule\_Thread} transition evaluates this priority queue and all the component message queues to find the highest priority component thread that is currently executing an operation or is ready to execute a new operation. If multiple component threads of equal (and highest) priority are ready to execute, then one of these threads is chosen at random and scheduled. When this thread consumes its quantum of CPU, the thread is enqueued to the back of the priority queue of threads and is not chosen again until all other competing threads are allowed a slice of CPU time. 


%The DREMS OS uses a fixed-priority preemptive scheduling scheme (constrained by temporal partitioning) to schedule component threads. %If multiple threads belonging to the same temporal partition are ready, the highest priority thread is always chosen for execution. If there is more than one candidate thread, one is chosen at random, followed thereafter by a Round-Robin scheduling scheme. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=\textwidth]{source-thread_execution_cycle}
	\caption{Component Thread Execution Cycle}
	\label{fig:Thread_Execution}
\end{figure}

If the highest priority (OS schedulable) thread is not already servicing an operation request, the next ready operation from the message queue is dequeued and scheduled for execution (represented by $O_{CT}$). Depending on the component scheduler, this operation may be the highest priority, or may have the earliest deadline or may be the oldest request. The scheduled thread token is placed in \emph{Running Threads}. %The guard on \emph{Schedule Thread} ensures that the highest priority thread in the currently running partition is scheduled first. 

When a thread token is marked as running, the model checks to see if the thread execution has any effect on itself or on other threads. These state changes are updated using the transition \emph{Execute\_Thread} which also handles time progression. Keeping track of $Value_{NC}$, the thread is preempted at each clock tick. This transition loop i.e. \emph{Schedule\_Thread -> Execute\_Thread -> Preempt/Unblock\_Thread -> Schedule\_Thread ...} cycle repeats forever, as long as there are no system-wide deadlocks and some upper limit on the clock values isn't reached.

\subsection{Modeling Component Operations}
\label{sec:Modeling_Component_Operations}
Every operation request \emph{O} made on a component \emph{$C_x$} is modeled as a Standard ML \emph{record} of the 4-tuple:

\begin{equation}
O(C_x) = \ < ID_O, \ Prio_O, \ Dl_O, \  Steps_O >
\end{equation}

where, $ID_O$ is a unique concatenation of strings that help identify and locate this operation in the system (consisting of the name of the operation, the component, the computing node, and the temporal partition). Assuming a PFIFO operation scheduling scheme, the operation's priority ($Prio_O$) is used by the analysis engine to enqueue this operation request on the message queue of $C_x$. The completion of this enqueue implies that this operation has essentially been \emph{scheduled} for execution. Figure \ref{fig:Operation_Scheduling} shows the operation scheduling cycle. The message queue tokens are placed in \emph{CMQ} (i.e. Component Message Queues). The tokens in this place are a constraint on the \emph{Schedule\_Thread} transition and are used to calculate the subset of threads that are \emph{runnable}. When threads execute, operation requests may be enqueued in component message queues as shown by this cycle. The transition  \emph{Enqueue\_Operation} enqueues any operation requests that are sent by the currently executing threads. These requests are enqueued onto the appropriate message queue in the chosen scheduling scheme. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=\textwidth]{cpn_cmq}
	\caption{Component Operation Scheduling Cycle}
	\label{fig:Operation_Scheduling}
\end{figure}

Once enqueued, if this operation does not execute and complete before its fixed deadline ($Dl_O$), its real-time requirements are violated. The completion of each operation request is saved in the place \emph{Completed\_Operations}. This place is an \emph{observer} that is simply notified by the \emph{Execute\_Thread} transition every time the thread execution also completes the executing operation. The tokens in Completed\_Operations saves a snapshot of the operation's properties including its operation's enqueue timestamp, dequeue timestamp, completion timestamp and deadline. By analyzing the tokens in this place, deadline violations in an interval of time can be detected.  

Once an operation request is dequeued, the execution of the operation is modeled as a transition system that runs through a sequence of \emph{steps} dictating its behavior. Any of these underlying steps can have a state-changing effect on the thread executing this operation e.g. interactions with I/O devices on the component-level could block the executing thread (for a non-deterministic amount of time) on the OS-level. Therefore, every component operation has a unique list of steps ($Steps_O$) that represent the sequence of local or remote interactions undertaken by the operation. Each of the \emph{m} steps in $Steps_O$ is a 4-tuple:

\begin{equation}
s_i = \ <Port, \ Unblk_{s_i}, \ Dur_t, \ Exec_t>
\end{equation}

where $1 \le i \le m$. \emph{Port} is a \emph{record} representing the exact communication port used by the operation during $s_i$. $Unblk_{s_i}$ is a list of component threads that are unblocked when $s_i$ completes. This list is used, e.g., when the completion of a synchronous remote method invocation on the server side is expected to unblock the client thread that made the invocation. Finally, temporal behavior of $s_i$ is captured using the last two integer fields: \emph{$Dur_t$} is the worst-case estimate of the time taken for $s_i$ to complete and $Exec_t$ is the relative time of the execution of $s_i$, with $0 \le Exec_t \le Dur_t$.

Consider the simple RMI application show in Figure \ref{fig:rmi_application}. The application has two components, a client and a server. The client component is associated with a periodic timer that triggers a sequence of interactions between the two components. When the client timer expires, a timer operation is enqueued on the client's operation queue. When scheduled, the client executor thread executes this operation, which makes an RMI call to the server component. Once the query is made, the client thread is effectively blocked till a response is received. The server thread that produces this response may not be scheduled immediately due to the constraints of temporal partition scheduling and other thread scheduling delays. Once the RMI operation is completed on the server, the client thread is unblocked. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{rmi_application}
	\caption{RMI Application}
	\label{fig:rmi_application}
\end{figure}

In the above example, the duration of time for which the client is blocked, is dependent on, among several factors, what happens inside the remote method on the server. This remote method could either simply take up CPU time, interact with the underlying framework or interact with other components in the application. To capture such interaction patterns, the \emph{step} color-set is defined in CPN. In this example, two \emph{operation} tokens are required to describe the operations handled by the components: a client side timer operation and a server side RMI operation. A sample client timer operation is shown in Figure \ref{fig:cpn_rmi_app_timer_token}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{cpn_rmi_app_timer_token}
	\caption{RMI Application - Client Timer Operation}
	\label{fig:cpn_rmi_app_timer_token}
\end{figure}

This timer operation runs on the client component with a priority of 50, and a deadline of 80 ms. The business logic of this operation consists of a single RMI call that takes 4 ms to send out the query after which it blocks the executing client thread. After the client thread runs for time \emph{t = q\_t}, the client thread is moved to a blocked state and an RMI operation is induced on the server side. The client side thread remains blocked until the server thread completes executing the remote method. Once the server thread completes execution, it sends the response of the RMI back to the client. The model takes note of how long the client has been blocked by using the time stamp at which it receives a response. The client thread runs for an additional 4 ms to process this response before it marks completion. The token for the server RMI operation is shown in Figure \ref{fig:cpn_rmi_app_server_token}. Note that all time measurements in this token are in micro-seconds i.e. a step duration of 4000 implies 4 ms of activity. The requested RMI operation is run on the server component with a priority of 50 and a deadline of 15 ms. The deadline of this operation cannot be worse than the deadline of the client side operation that initiated the interaction. If this operation delays past 80 ms, a client side deadline violation is detected as the client thread is blocking for longer than expected. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{cpn_rmi_app_server_token}
	\caption{RMI Application - Server Operation}
	\label{fig:cpn_rmi_app_server_token}
\end{figure}


\subsection{Modeling Component Interactions}
\label{sec:operation_induction}

In our earlier RMI example, the client is periodically triggered by a timer to make a remote method call to the server. When the client executes an instance of this timer-triggered operation, a related operation request is enqueued on the server's message queue. In reality, this is handled by the underlying middleware. Since the details of this framework are not modeled, the server-side request is captured as an \emph{induced operation} that manifests as a consequence of the client-side activity. Tokens that represent such design-specific interactions are maintained in the place \emph{Component Interactions} (Figures \ref{fig:hlcpn},\ref{fig:cpn_iop}) and modeled as shown in equation \ref{eq:component_interactions}. The interaction \emph{Int} observed when a component $C_x$ queries another component $C_y$ is modeled as the 3-tuple:

\begin{equation}
\label{eq:component_interactions}
Int(C_x, C_y) = \ < Node_{C_x}, \ Port_{C_x}, \ O(C_y)>
\end{equation}

When an operational \emph{step} in component $C_x$ uses port $Port_{C_x}$ to invoke an operation on component $C_y$, the request $O_{C_y}$ is enqueued on the message queue of $C_y$. 

 \begin{figure}[ht]
 	\centering
 	\includegraphics[width=\textwidth]{component_interactions}
 	\caption{Operation Induction}
 	\label{fig:cpn_iop}
 \end{figure} 
 
Every \emph{interaction} token contains an interaction port and an operation. The transition \emph{Execute\_Thread} observes the activity on the currently running thread. When executing the model, if a particular step executed by a component thread would, on completion, request the services of another component, a token is placed on the \emph{Waiting to Enqueue} place. So, once the client thread pushes out an RMI query, an operation needs to be induced on the server queue. So an \emph{interaction} token for this communication is constructed. The model waits for the RMI call on the client side to complete, at which point it places the operation \emph{i\_op} on the server message queue. This induction is represented in Figure \ref{fig:cpn_rmi_app_iop}. 

 \begin{figure}[h]
 	\centering
 	\includegraphics[width=\textwidth]{cpn_rmi_app_iop}
 	\caption{Operation Induction Token}
 	\label{fig:cpn_rmi_app_iop}
 \end{figure}

\subsection{Modeling Timers}

DREMS components are inactive initially; once deployed, a component executor thread is not eligible to run until there is a related operation request in the component's message queue. To start a sequence of component interactions, periodic or sporadic timers can be used to trigger a component operation. In CPN, each timer $TMR$ is held in the place \emph{Timers} and represented as shown in Eq. \ref{eq:TMR}. Timers are characterized by a period ($Prd_{TMR}$) and an offset ($Off_{TMR}$). Every timer triggers a component using the operation request $O_{TMR}$.

\begin{equation}
\label{eq:TMR}
TMR = \ < Prd_{TMR}, \ Off_{TMR}, \ O_{TMR}>
\end{equation} 

When the component's timer expires, a timer callback operation is placed on the component message queue. When the component executor thread is picked by the OS scheduler, this operation is dequeued and the timer callback is executed. In CPN, timer operations are modeled as shown in Figure \ref{fig:cpn_timers}. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{timers}
	\caption{Timer Operations}
	\label{fig:cpn_timers}
\end{figure}

All component timers are expressed as separate tokens and initialized in the \emph{Timers} place. It is important to note that the enqueue operation does not happen until the appropriate partition is active. This is because the component-specific thread responsible for enqueueing (or dequeuing) incoming operations is also affected by temporal partitioning. 

\section{Modeling Component Operation Business Logic}
\label{sec:BL_Model}

\subsection{Problem Statement}

Consider a set of component-based applications deployed on distributed hardware. Each application consists of groups of components that interact with each other and also with the external environment e.g. I/O devices, other applications, underlying middleware etc. Each component exposes a set of interfaces through which external entities can request \emph{operations}. As mentioned earlier, an operation is an abstraction for the different tasks undertaken by a component. These operations are exposed through ports and can be requested by other components. When an operation is requested, the request is placed in the component's message queue and eventually serviced. When ready, the business logic of the operation i.e. a local callback is executed. This piece of code represents the brains of the operation. The goal here is to be able to model this business logic, for every component operation, effectively as part of the design model, including temporal estimates such as worst-case execution times for individual code blocks, so that the model can be translated into appropriate data structures in our CPN analysis model.

\subsection{Challenges}

The execution of component operations service the various periodic or aperiodic interaction requests coming from either a timer or other connected (possibly distributed) components. Each operation is written by an application developer as a sequence of execution steps. Each step could execute a unique set of activities, e.g. perform a local calculation or a library call, initiate an interaction with another component, process a response from external entities, and it can have data-dependent, possibly looping control flow, etc. The behavior derived by the combination of these steps contribute to the worst-case execution of the component operation. The behavior may include non-deterministic delays due to component interactions while being constrained by the temporally partitioned scheduling scheme and hardware resources. The challenge here is to identify a grammar that would enable the description of potentially dynamic behavior realized in a component operation. The modeling aspects emerging from this challenge will have to propagate to any timing analysis model that studies the system. This is true because any non-deterministic delays such as blocking times need to be accounted for when analyzing the temporal behavior.

\subsection{Outline of Solution}
The business-logic model of a component operation requires to be completely integrated into our CPN modeling methodology. This means that the model, however complex, needs to be translated into some token data structure in CPN. This is our primary constraint. The CPN analysis model needs to know how an operation is structured i.e. what are the sequential steps in the code, along with WCET on each step. Lastly, since the CPN model does not model or simulate component data management, data-dependent conditional statements in the business-logic model were avoided or abstracted away. Following these rules, we designed a language for describing the component operation business logic. Each component operation model is then attached to a component port or timer in the main design model and enriches the model with refined details about the workings of the operation. In summary, this model is capable of representing several types of code blocks including local function calls, remote procedure calls, outgoing port-to-port interactions, incoming port-response processing, and bounded loops.

The execution of component operations service the various periodic or aperiodic interaction requests coming from either a timer or other connected (possibly distributed) components. Each operation is written by an application developer as a sequence of execution \emph{steps}. Each step could execute a unique set of activities, e.g. perform a local calculation or a library call, initiate an interaction with another component, process a response from external entities, and it can have data-dependent, possibly looping control flow, etc. The behavior derived by the combination of these steps contribute to the worst-case execution of the component operation. The behavior may include non-deterministic delays due to component interactions while being constrained by the  temporally partitioned scheduling scheme and hardware resources. This section briefly describes the various aspects of this behavior specification that are general enough to be applicable to a range of component-based systems.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./img/BL-EBNF}
	\caption{Grammer for the Business Logic of Component Operations}
	\label{fig:ebnf}
\end{figure}

Figure \ref{fig:ebnf} shows the Extended Backus-Naur form representation of the grammar \cite{SEUS} used for modeling the business logic of component operations. The symbol \emph{ID} represents identifiers, a unique grouping of alphanumeric characters, and the symbol \emph{INT} represents positive integers. Each operation is characterized by a unique name, a priority, and a deadline. The priority is an integer used to resolve scheduling conflicts between operations \emph{provided} by the same component when multiple messages from other entities are received. This priority is different from the executor thread's OS-level priority; operation's priorities are used by the component-level scheduler to find the next operation to be executed by a component executor thread. The deadline of the operation is the worst-case time that can elapse after the operation is marked as \emph{ready} and the completion of the operation. The business logic of every component operation is modeled as a sequence of steps, each with known worst-case execution time. We broadly classify these steps into (1) blocks of sequential code, (2) peer-to-peer synchronous and asynchronous remote calls, (3) anonymous publish/subscribe distribution service calls, (4) blocking and non-blocking I/O interactions and (5) bounded loops.

% May want to mention that this is different from the executor thread's priority - that is used once the operation is running on the CPU.  

Model conforming to this grammar represent the sequence of steps that can be executed in a component operation. These component operations are executed when the component is triggered which can happen in three ways: (1) the expiry of a timer executes a callback, (2) the reception of message on the subscriber port, and (3) the reception of a method request from a remote client on a server port. When one of these events occurs, the corresponding operation is enqueued on the message queue and eventually handled. When the operation is executed, the component can use any and all outgoing ports at its disposal to publish messages or query other remote servers, as shown in Figure \ref{fig:ebnf}. Additionally, the operation can also execute local non-blocking function calls that perform any required computations. For each such step, the grammar enables the integration of timing properties. With RMI calls, \emph{query\_time} is an optional annotation that represents the worst-case estimated duration of time taken by a client port to send out a request to the serving component. This time can be used to include network buffering delays or any other pre-processing steps enforced by the infrastructure before the request actually leaves the client component. Similarly, \emph{processing\_time} represents the duration of time after the client component receives the response from the server when any post-processing steps are executed by the infrastructure. These delays are optional and can be encapsulated within \emph{LOCAL} blocks on either side of the call. If these expected delays are set to zero, the analysis will execute these interactions in a single synchronous step taking no time. However, in reality these steps still take a non-zero amount of time to execute. Therefore, if such metrics are not known then these values can be set to zero and an overall worst-case execution time can be set per operation. This is the maximum amount of time that can elapse after the component operation has begun to execute. This time will include all component interactions and network delays that affect the operation's execution. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{./img/abstract_business_logic}
	\caption{Sample Business Logic Model}
	\label{fig:sblm}
\end{figure}

Figure \ref{fig:sblm} shows a sample business logic model conforming to this grammar. The Data Acquisition Module is a periodically triggered I/O component i.e. this component receives a stream of sensor information from various sensor devices e.g. inertial measurement units (IMU) and GPS modules. This component packages this information and publishes sensor state as a message to all subscribers e.g. components that implement controllers. Figure \ref{fig:sblm} shows the translation from the conceptual understanding of the workings of this component operation to the abstract business logic model that is then translated into CPN tokens (Figure \ref{fig:blte}), as described in Section \ref{sec:Modeling_Component_Operations}. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./img/cpn_business_logic_token_example}
	\caption{CPN Business Logic Representation}
	\label{fig:blte}
\end{figure}

Every time the above timer expires, this \emph{sensor\_read\_timer} operation is enqueued onto the component message queue (Figure \ref{fig:Operation_Scheduling}). When the \emph{Data\_Acquisition\_Module} component is scheduled, this operation is dequeued from the message queue and is marked for execution. The \emph{Execute\_Thread} transition fires, this component thread executes i.e. each operation step in the component operation proceeds to sequentially consume CPU. So, first the LOCAL code block executes for 15 milliseconds. Every 4 milliseconds (the default clock tick), the scheduler checks to see if this thread needs to be preempted in favor of any other higher priority ready thread. If no other higher priority threads are ready to execute, this thread continues to consume CPU and the \emph{exec\_time} attribute of this step is incremented. When $exec\_time = duration$, this step is removed from the operation and the next step begins execution. Here, the component uses its publisher port to send out a sensor\_state message. Since no other delays are detailed for this step, the analysis model executes this step in zero time. When this step completes, the \emph{Execute\_Thread} transition enqueues an operation on all subscriber components' message queues following the interaction rules described in Section \ref{sec:operation_induction}. Lastly, a LOCAL code block consumes 2 milliseconds of CPU and the operation is marked as complete. The clock value on this node when the operation completes is saved as a completion\_time of this operation.  


\section{Modeling Component-based Cyber-Physical Systems}

A large subset of modern DRE systems are also Cyber-Physical Systems (CPS) e.g. flight controllers \cite{sharp1998reducing}, traffic light controllers \cite{huber1998traffic} etc. In these systems, the runtime application can consist of a number of sub-systems including a list of sensors, actuators, and controllers that interact with each other and govern the dynamics of the \emph{plant} i.e. the physical properties of the system. For instance, a DREMS-style sensor component in a flight control scenario may interact with low-level hardware e.g. GPS, inertial measurement units, cameras etc., receive a stream of information, sample sensor state from this stream and publish this information at a defined period. A high-level controller may receive this sensor data periodically and perform some PID control, commanding a remote actuator component with new settings e.g. the deflection of control surfaces, that in turn cause changes in the attitude of an aircraft, elevations in pitch etc. The actuator periodically receives these commands and appropriately modifies the state of the low-level hardware e.g. driving a set of motors for a defined duration of time so that an aircraft control surface causes airplane lift-off. 

% This is not a very good explanation of the difference between the two cases. Clarify it.
% In the first case the sensor software component operation is triggered (by a timer, for example), then it actively queries the sensor -- this query can be blocking or non-blocking. 
% In the second case the component is triggered by the arrival of the sensor data (that is streaming) and then it publishes that received data. 

When modeling CPS sensors, the analysis considers two cases: request-response style queries and periodic sensor streams. When using request-response queries, a sensor software component operation is triggered e.g. through a component timers, and this operation actively queries the sensor hardware for updated state. The query can be blocking or non-blocking, and the sensor software component publishes this updated state to other interested components. When using periodic sensor streams, the component is triggered by the arrival of sensor data (that is streaming) and the component publishes the received data sample. 

Similarly, an actuator component may receive commands to actuate e.g. deflecting the flaps on the trailing edge of an aircraft wing. This is the second \emph{I/O component} that directly interacts with hardware and influences the state of the system. For the sake of modeling simplicity, actuation commands are modeled as non-blocking write operations performed by software components to control low-level hardware. By modeling these interactions as separate from other local code blocks or function calls, the business logic model is enriched with a broader set of categories that can be used to identify the source of timing delays in a component operation. This section describes the integration of these designs into both the business logic model and the CPN analysis. 

Figure \ref{fig:sensors} shows the integration of CPS sensor models into the CPN analysis model. The sensor hardware is modeled in the \emph{Sensor} place; each sensor token is associated with a period representing the periodicity of the state updates. Each sensor state is updated by the \emph{Sensor\_Update} transition periodically and a new \emph{Sensor\_State} is recorded. This sensor state is either queried by a component operation or received as part of a sensor stream. In case of blocking read operations, the querying operation blocks until a new sensor state is available in the Sensor\_State place. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./img/sensors}
	\caption{CPN Analysis Model - CPS Sensors}
	\label{fig:sensors}
\end{figure}
\FloatBarrier

The business logic of the component operations can read the state of the variables in \emph{Sensor\_State} as required when performing computations. Since the CPN analysis does not model data or data flow, only the name of the required sensors is captured in the business logic. Thus, the business logic model, as described in Section \ref{sec:BL_Model} is revised as shown in Figure \ref{fig:cps_bl}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{./img/cps_bl}
	\caption{Modeling CPS - Business Logic Integration}
	\label{fig:cps_bl}
\end{figure}
\FloatBarrier

The business logic model integrates two types of sensor read operations and one actuator write operation. The sensor read operation can be blocking or asynchronous. In blocking reads, the operation does not continue execution if the required sensor state is unavailable in \emph{Sensor\_State} place. When a new state is available, the currently blocked operation consumes the sensor token and proceeds execution i.e. the worst-case time taken to perform the sensor read is dependent on the periodicity of the sensor update. This is similar to a request-response style query i.e. a sensor component requests updated state information from a sensor device, and the sensor responds as soon as it is updated. In asynchronous read operations, if an updated sensor state is unavailable, the operation uses the latest state i.e. the previous state of the sensor obtained from a previous read operation. Asynchronous sensor read operations do not take any time and the CPN analysis does not assume any faulty behavior on the part of the sensors i.e. the sensor state is strictly updated periodically without any delays. As for actuators, the most common means to control an actuator e.g. servo motor, is to write a value to the corresponding actuator system variable(s) e.g. toggling the state of a set of general purpose input-output (GPIO) hardware pin(s). Such system variables or hardware pins are assumed to be always ready to accept new write operations. 

The above models are quite simplistic. There is no way for the analysis model to know if the received state information is stale or not. The component operation, at runtime, may decide not to publish sensor state if the received state information is stale. Such a choice cannot be modeled with this business logic since there is neither model of data (values of local variables or sensor state) nor any model of conditional statements. Thus, in such cases, the results of the analysis can lead to gross overestimates as the analysis will only consider the case when the publish always happens i.e. the worst-case scenario. 


  

